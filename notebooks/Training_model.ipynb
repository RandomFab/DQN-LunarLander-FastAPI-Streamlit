{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb5ca24",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f26651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installation des d√©pendances pour Google Colab\n",
    "# # On utilise [extra] pour SB3 (TensorBoard support) et [box2d] pour l'environnement LunarLander\n",
    "# !pip install \"stable-baselines3[extra]\" \"gymnasium[box2d]\" tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b66eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00732a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95632fe1",
   "metadata": {},
   "source": [
    "# EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a180885",
   "metadata": {},
   "source": [
    "## LunarLander environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652c01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LunarLander_env = gym.make('LunarLander-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745b311",
   "metadata": {},
   "source": [
    "## LunarLander online documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb9c28",
   "metadata": {},
   "source": [
    "### <u>Description</u>\n",
    "This environment is a classic rocket trajectory optimization problem. According to Pontryagin‚Äôs maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.  \n",
    "![Animation du Lander](https://gymnasium.farama.org/_images/lunar_lander.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8f3c8",
   "metadata": {},
   "source": [
    "### <u>Actions</u> \n",
    "\n",
    "There are four discrete actions available:\n",
    "\n",
    "- 0: do nothing\n",
    "- 1: fire left orientation engine\n",
    "- 2: fire main engine\n",
    "- 3: fire right orientation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7121b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space : Discrete(4)\n",
      "Observation shape : ()\n",
      "Observation sample : 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation Space : {LunarLander_env.action_space}\")\n",
    "print(f\"Observation shape : {LunarLander_env.action_space.shape}\")\n",
    "print(f\"Observation sample : {LunarLander_env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cc662",
   "metadata": {},
   "source": [
    "### <u>Environment</u>\n",
    "\n",
    "The state is an 8-dimensional vector:\n",
    "\n",
    "- Coordinates of the lander in x & y (2)\n",
    "- Linear velocities in x & y (2)\n",
    "- angle (1)\n",
    "- angular velocity (1)\n",
    "- two booleans that represent whether each leg is in contact with the ground or not. (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27a08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space : Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Observation shape : (8,)\n",
      "Observation sample : [ 1.3324467   1.917008   -1.4704447  -3.2587903  -5.6439013   8.102522\n",
      "  0.78382874  0.8964716 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation Space : {LunarLander_env.observation_space}\")\n",
    "print(f\"Observation shape : {LunarLander_env.observation_space.shape}\")\n",
    "print(f\"Observation sample : {LunarLander_env.observation_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97badfab",
   "metadata": {},
   "source": [
    "### <u>Rewards</u> \n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "- is increased/decreased the closer/further the lander is to the landing pad.\n",
    "- is increased/decreased the slower/faster the lander is moving.\n",
    "- is decreased the more the lander is tilted (angle not horizontal).\n",
    "- is increased by 10 points for each leg that is in contact with the ground.\n",
    "- is decreased by 0.03 points each frame a side engine is firing.\n",
    "- is decreased by 0.3 points each frame the main engine is firing.\n",
    "- The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "An episode is considered a solution if it scores at least 200 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0d780",
   "metadata": {},
   "source": [
    "**We will choose de discrete version of LunarLander.**  \n",
    "**It will allows us to explore in an easy way how the environment adn action behave.**  \n",
    "**We will for sure use DQN model (which is discrete) to control the spaceship** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e70192",
   "metadata": {},
   "source": [
    "## DQN model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528445f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN_model = DQN(\n",
    "#     policy = \"MlpPolicy\", # classic nn\n",
    "#     env = LunarLander_env, \n",
    "#     verbose = 1,\n",
    "#     learning_rate = 1e-3,\n",
    "#     buffer_size=50000,\n",
    "#     learning_starts=5000,\n",
    "#     exploration_fraction=0.6,\n",
    "#     exploration_final_eps=0.05,\n",
    "#     device=\"auto\"\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c124913",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d79bf",
   "metadata": {},
   "source": [
    "## Training DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d94644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"lancement de l'entrainement...\")\n",
    "\n",
    "# DQN_model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec01f11",
   "metadata": {},
   "source": [
    "## Evaluate baseline DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa96d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluation en cours...\")\n",
    "# mean_reward, std_reward = evaluate_policy(DQN_model,LunarLander_env,n_eval_episodes=50)\n",
    "# print(f\"Moyenne {mean_reward} | +- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329892c",
   "metadata": {},
   "source": [
    "**Le mod√®le de base obtient un reward de -102 ¬± 139. Ce score servira de r√©f√©rence pour l'optimisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3827b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b11c3",
   "metadata": {},
   "source": [
    "# OPTIMISATION HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161ac15",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edececfe",
   "metadata": {},
   "source": [
    "### Baseline params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c457bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_LR = 1e-3\n",
    "BASELINE_BS = 50000\n",
    "BASELINE_EF = 0.6\n",
    "BASELINE_G = 0.99\n",
    "BASELINE_LS = 5000\n",
    "TIME_STEPS = 300000 # Augment√© de 25k √† 100k pour une meilleure visibilit√© des performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59389391",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../logs/dqn_LunarLander_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74971f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimized_DQN_model = DQN(    \n",
    "    policy = \"MlpPolicy\",\n",
    "    env = LunarLander_env, \n",
    "    verbose = 0,\n",
    "    tensorboard_log = log_dir,\n",
    "    learning_rate = BASELINE_LR,\n",
    "    buffer_size=BASELINE_BS,\n",
    "    learning_starts=BASELINE_LS,\n",
    "    exploration_fraction=BASELINE_EF,\n",
    "    gamma= BASELINE_G,\n",
    "    device=\"cuda\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93631ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x17e34c8dfd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_DQN_model.learn(total_timesteps=TIME_STEPS, tb_log_name=\"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b98ba8",
   "metadata": {},
   "source": [
    "## Learning rate optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5751a",
   "metadata": {},
   "source": [
    "### Learning rate to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb5dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-1, 1e-2, 1e-4, 1e-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c39e1",
   "metadata": {},
   "source": [
    "### Learning rates loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f63258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Fabien\\Desktop\\OC\\P11\\AstroDynamics\\.venv\\Lib\\site-packages\\rich\\live.py:260: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Fabien\\Desktop\\OC\\P11\\AstroDynamics\\.venv\\Lib\\site-packages\\rich\\live.py:260: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a learning rate set at 0.1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a learning rate set at 0.01 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a learning rate set at 0.0001 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a learning rate set at 1e-05 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for learning_rate in learning_rates:\n",
    "    run_name = f\"lr_{learning_rate}\"\n",
    "    print(f\" --- Start learning for a learning rate set at {learning_rate} ---\")\n",
    "\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        LunarLander_env,\n",
    "        learning_rate=learning_rate,\n",
    "        buffer_size=BASELINE_BS,\n",
    "        learning_starts=BASELINE_LS,\n",
    "        gamma= BASELINE_G,\n",
    "        exploration_fraction = BASELINE_EF,\n",
    "        device=\"cuda\",\n",
    "        tensorboard_log=log_dir\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TIME_STEPS,tb_log_name=run_name, progress_bar=True)\n",
    "\n",
    "    model.save(f\"../data/model/dqn_lunarlander_{run_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8243ec2",
   "metadata": {},
   "source": [
    "## Exploration_fraction optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caef7e8",
   "metadata": {},
   "source": [
    "### Exploration fraction to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e609ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_fractions = [0.1,0.4,0.7,0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3e947",
   "metadata": {},
   "source": [
    "### exploration fraction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a exploration fraction set at 0.1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a exploration fraction set at 0.4 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a exploration fraction set at 0.7 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a exploration fraction set at 0.9 ---\n"
     ]
    }
   ],
   "source": [
    "for exploration_fraction in exploration_fractions:\n",
    "    run_name = f\"ef_{exploration_fraction}\"\n",
    "    print(f\" --- Start learning for a exploration fraction set at {exploration_fraction} ---\")\n",
    "\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        LunarLander_env,\n",
    "        learning_rate=BASELINE_LR,\n",
    "        buffer_size=BASELINE_BS,\n",
    "        learning_starts=BASELINE_LS,\n",
    "        gamma= BASELINE_G,\n",
    "        exploration_fraction = exploration_fraction,\n",
    "        device=\"cuda\",\n",
    "        tensorboard_log=log_dir\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TIME_STEPS,tb_log_name=run_name, progress_bar=True)\n",
    "\n",
    "    model.save(f\"../data/model/dqn_lunarlander_{run_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680d659",
   "metadata": {},
   "source": [
    "## Gamma optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d8461",
   "metadata": {},
   "source": [
    "### Gamma to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e166ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8,0.9,0.95,0.97]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f25e8",
   "metadata": {},
   "source": [
    "### gamma loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a gamma set at 0.8 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Fabien\\Desktop\\OC\\P11\\AstroDynamics\\.venv\\Lib\\site-packages\\rich\\live.py:260: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Fabien\\Desktop\\OC\\P11\\AstroDynamics\\.venv\\Lib\\site-packages\\rich\\live.py:260: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a gamma set at 0.9 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a gamma set at 0.95 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a gamma set at 0.97 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for gamma in gammas:\n",
    "    run_name = f\"g_{gamma}\"\n",
    "    print(f\" --- Start learning for a gamma set at {gamma} ---\")\n",
    "\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        LunarLander_env,\n",
    "        learning_rate=BASELINE_LR,\n",
    "        buffer_size=BASELINE_BS,\n",
    "        learning_starts=BASELINE_LS,\n",
    "        gamma= gamma,\n",
    "        exploration_fraction = BASELINE_EF,\n",
    "        device=\"cuda\",\n",
    "        tensorboard_log=log_dir\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TIME_STEPS,tb_log_name=run_name, progress_bar=True)\n",
    "\n",
    "    model.save(f\"../data/model/dqn_lunarlander_{run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da9672",
   "metadata": {},
   "source": [
    "## Buffer_size optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa4acf",
   "metadata": {},
   "source": [
    "### buffer size to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6c934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_sizes = [10000,50000,100000,500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0e75a",
   "metadata": {},
   "source": [
    "### buffer size loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3e277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a buffer size set at 10000 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a buffer size set at 50000 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a buffer size set at 100000 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Start learning for a buffer size set at 500000 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for buffer_size in buffer_sizes:\n",
    "    run_name = f\"bs_{buffer_size}\"\n",
    "    print(f\" --- Start learning for a buffer size set at {buffer_size} ---\")\n",
    "\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        LunarLander_env,\n",
    "        learning_rate=BASELINE_LR,\n",
    "        buffer_size=buffer_size,\n",
    "        learning_starts=BASELINE_LS,\n",
    "        gamma= BASELINE_G,\n",
    "        exploration_fraction = BASELINE_EF,\n",
    "        device=\"cuda\",\n",
    "        tensorboard_log=log_dir\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TIME_STEPS,tb_log_name=run_name, progress_bar=True)\n",
    "\n",
    "    model.save(f\"../data/model/dqn_lunarlander_{run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b196e",
   "metadata": {},
   "source": [
    "## Best hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc479c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_HP_DQN = DQN(    \n",
    "    policy = \"MlpPolicy\",\n",
    "    env = LunarLander_env, \n",
    "    verbose = 0,\n",
    "    tensorboard_log = log_dir,\n",
    "    learning_rate = 1e-3,\n",
    "    buffer_size=100000,\n",
    "    learning_starts=BASELINE_LS,\n",
    "    exploration_fraction=0.2,\n",
    "    gamma= 0.99,\n",
    "    device=\"cuda\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9152357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fabien\\Desktop\\OC\\P11\\AstroDynamics\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path '..\\data\\model' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "best_HP_DQN.learn(total_timesteps=1000000, tb_log_name=\"best_HP\")\n",
    "# On utilise un nom de fichier explicite pour √©viter d'utiliser le run_name de la boucle pr√©c√©dente\n",
    "best_HP_DQN.save(\"../data/model/dqn_lunarlander_best_HP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac68ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7ca22",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d1983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du meilleur mod√®le\n",
    "model_path = \"../data/model/dqn_lunarlander_best_HP\"\n",
    "loaded_model = DQN.load(model_path, env=LunarLander_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"√âvaluation du mod√®le charg√© en cours...\")\n",
    "mean_reward, std_reward = evaluate_policy(loaded_model, LunarLander_env, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"Reward moyen : {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845c0a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18032302",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e635fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mission termin√©e ! Score final : 267.67\n"
     ]
    }
   ],
   "source": [
    "# On recr√©e l'environnement avec le mode 'human' pour voir la fen√™tre s'ouvrir\n",
    "visual_env = gym.make('LunarLander-v3', render_mode='human')\n",
    "\n",
    "observation, info = visual_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Cette commande ouvre et met √† jour la fen√™tre externe\n",
    "    visual_env.render()\n",
    "    \n",
    "    action, _ = loaded_model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, info = visual_env.step(action=action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Mission termin√©e ! Score final : {total_reward:.2f}\")\n",
    "visual_env.close() # Ferme la fen√™tre proprement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e976f",
   "metadata": {},
   "source": [
    "## üé• Enregistrement GIF de l'atterrissage\n",
    "Enregistre un √©pisode complet en `.gif` dans le dossier `images/` pour l'int√©grer au README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53c782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpisode termin√© | Score : 260.44 | Frames captur√©es : 270\n",
      "‚úÖ GIF sauvegard√© : ../images\\landing_demo.gif\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import os\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"../images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "gif_path = os.path.join(output_dir, \"landing_demo.gif\")\n",
    "\n",
    "# Environnement en mode rgb_array pour capturer les frames\n",
    "record_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "observation, info = record_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    frame = record_env.render()\n",
    "    frames.append(frame)\n",
    "    \n",
    "    action, _ = loaded_model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, info = record_env.step(action=action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "record_env.close()\n",
    "print(f\"√âpisode termin√© | Score : {total_reward:.2f} | Frames captur√©es : {len(frames)}\")\n",
    "\n",
    "# Conversion des frames en GIF anim√© avec Pillow\n",
    "imgs = [PIL.Image.fromarray(f) for f in frames]\n",
    "imgs[0].save(\n",
    "    gif_path,\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    duration=33,  # ~30 FPS\n",
    "    loop=0\n",
    ")\n",
    "print(f\"‚úÖ GIF sauvegard√© : {gif_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
